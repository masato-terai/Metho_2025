---
title: "重回帰分析における検定の多重性について"
author: "寺井雅人"
output: html_document
date: "Published:2025-03-27, Last update(JST): `r format(Sys.time(), '%Y-%m-%d %X')`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
```

# シミュレーション

```{r}
set.seed(123)  # 再現性確保
n_sim <- 10000  # シミュレーション回数
n_per_group <- 50  # 各グループのサンプルサイズ
alpha <- 0.05  # 有意水準
```

# 1要因2水準モデル

## 係数の数 = 1（切片を除く）

```{r}
results_1factor2 <- replicate(n_sim, {
  A <- factor(rep(c("X", "Y"), each = n_per_group))  # 2水準
  y <- rnorm(2 * n_per_group, mean = 0, sd = 2)  # 帰無仮説が正しい
  
  model <- lm(y ~ A)  # 線形回帰
  coef_count_excl_intercept <- length(coef(model)) - 1  # 切片を除いた係数の数
  p_value <- summary(model)$coefficients[2, 4]  # Aのp値
  
  c(p_value, coef_count_excl_intercept)  # ベクトルで返す
})

# 結果をデータフレームに変換
results_df_1factor2 <- data.frame(
  p_value = results_1factor2[1, ],  # p値
  coef_count = results_1factor2[2, ]  # 切片を除いた係数の数
)

```

## Type 1 Errorの割合

- 有意水準に近い値

```{r}
res_1_2 <- mean(results_df_1factor2$p_value < alpha)

res_df1_2 <- data.frame(
  type1 = res_1_2,
  coeff = unique(results_df_1factor2$coef_count)
)
```

# 1要因4水準モデル
## 係数の数 = 3（切片を除く）

- 3水準以上だと係数が複数になる。そこで係数の中で一番小さいp値を格納
```{r}
results_1factor4 <- replicate(n_sim, {
  A <- factor(rep(c("W", "X", "Y", "Z"), each = n_per_group))  # 4水準
  y <- rnorm(4 * n_per_group, mean = 0, sd = 2)  # 帰無仮説が正しい

  model <- lm(y ~ A)  # 線形回帰
  coef_count_excl_intercept <- length(coef(model)) - 1  # 切片を除いた係数の数
  anova_p <- summary(model)$coefficients[-1, 4]  # Aの各水準のp値（基準カテゴリを除く）
  min_p <- min(anova_p)  # 最小のp値
  
  c(min_p, coef_count_excl_intercept)  # ベクトルで返す
})

# 結果をデータフレームに変換
results_df_1factor4 <- data.frame(
  p_value = results_1factor4[1, ],  # 最小のp値
  coef_count = results_1factor4[2, ]  # 切片を除いた係数の数
)
```

## Type 1 Errorの割合

- 有意水準の倍

```{r}
res_1_4 <- mean(results_df_1factor4$p_value < alpha)

res_df_1_4 <- data.frame(
  type1 = res_1_4,
  coeff = unique(results_df_1factor4$coef_count)
)
```

# 2要因2水準モデル
## 係数の数 = 2（切片を除く）
- 3水準以上だと係数が複数になる。そこで係数の中で一番小さいp値を格納

```{r}
results_2factor2 <- replicate(n_sim, {
  A <- factor(rep(rep(c("X", "Y"), each = n_per_group), 2))  # 2水準
  B <- factor(rep(c("M", "N"), each = 2 * n_per_group)) # 2水準
  y <- rnorm(4 * n_per_group, mean = 0, sd = 2)  # 帰無仮説が正しい

  model <- lm(y ~ A + B)  # 交互作用なしモデル
  coef_count_excl_intercept <- length(coef(model)) - 1  # 切片を除いた係数の数
  anova_p <- summary(model)$coefficients[-1, 4]  # AとBのp値
  min_p <- min(anova_p)  # 最小のp値
  
  c(min_p, coef_count_excl_intercept)  # ベクトルで返す
})

results_df_4 <- data.frame(
  p_value = results_2factor2[1, ],  # 最小のp値
  coef_count = results_2factor2[2, ]  # 切片を除いた係数の数
)
```

## Type 1 Errorの割合
- 有意水準を超えた
```{r}
res_4 <- mean(results_df_4$p_value < alpha)

res_df_4 <- data.frame(
  type1 = res_4,
  coeff = unique(results_df_4$coef_count)
)
```

# 2 × 3（交互作用無し）
## 係数の数 = 3（切片を除く）
```{r}
results_2factor6 <- replicate(n_sim, {
  A <- factor(rep(rep(c("X", "Y"), each = n_per_group), 3))  # 2水準
  B <- factor(rep(c("M", "N", "O"), each = 2 * n_per_group)) # 3水準
  y <- rnorm(6 * n_per_group, mean = 0, sd = 2)  # 帰無仮説が正しい

  model <- lm(y ~ A + B)  # 交互作用なしモデル
  coef_count_excl_intercept <- length(coef(model)) - 1  # 切片を除いた係数の数
  anova_p <- summary(model)$coefficients[-1, 4]  # AとBのp値
  min_p <- min(anova_p)  # 最小のp値
  
  c(min_p, coef_count_excl_intercept)  # ベクトルで返す
})

results_df_6 <- data.frame(
  p_value = results_2factor6[1, ],  # 最小のp値
  coef_count = results_2factor6[2, ]  # 切片を除いた係数の数
)
```

## Type 1 Errorの割合
- 有意水準の倍

```{r}
res_6 <- mean(results_df_6$p_value < alpha)

res_df_6 <- data.frame(
  type1 = res_6,
  coeff = unique(results_df_6$coef_count)
)
```

# 最も係数が多いモデル

```{r}
results_complex <- replicate(n_sim, {
  # 2水準のカテゴリ変数（A, B, C, D）
  A <- factor(rep(c("X", "Y"), each = n_per_group))
  B <- factor(rep(c("M", "N"), each = n_per_group))
  C <- factor(rep(c("P", "Q"), each = n_per_group))
  D <- factor(rep(c("G", "H"), each = n_per_group))
  
  # 連続変数（X1, X2, ..., X10）
  continuous_vars <- as.data.frame(matrix(rnorm(10 * 2 * n_per_group, mean = 0, sd = 2), 
                                          ncol = 10))
  colnames(continuous_vars) <- paste0("X", 1:10)
  
  # 目的変数 y（帰無仮説が正しい）
  y <- rnorm(2 * n_per_group, mean = 0, sd = 2)
  
  # 線形回帰モデルの作成
  model <- lm(y ~ A + B + C + D + ., data = continuous_vars)  # すべての変数を回帰に含める
  
  # 切片を除いた係数の数
  coef_count_excl_intercept <- length(coef(model)) - 1
  
  # すべてのp値を取得（切片を除く）
  p_values <- summary(model)$coefficients[-1, 4]
  
  # 最小のp値を取得（最も小さいp値がType I error の影響を示すため）
  min_p_value <- min(p_values)
  
  c(min_p_value, coef_count_excl_intercept)  # ベクトルで返す
})

# 結果をデータフレームに変換
results_df_complex <- data.frame(
  p_value = results_complex[1, ],  # 最小のp値
  coef_count = results_complex[2, ]  # 切片を除いた係数の数
)
```

## Type 1 Errorの割合
- 有意水準の倍

```{r}
res_comp <- mean(results_df_complex$p_value < alpha)

res_df_comp <- data.frame(
  type1 = res_comp,
  coeff = unique(results_df_complex$coef_count)
)
```


# 係数とType 1 errorの関係

```{r}
sum_results <- rbind(res_df1_2, res_df_1_4, res_df_4, res_df_6, res_df_comp)
```

```{r}
sum_results
```


```{r}
sum_results %>%
  ggplot(aes(x = coeff, y = type1)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +  # loess回帰
  geom_hline(yintercept = 0.05, color = "red", linetype = "dashed") +
  ylab("The type 1 error rate") +
  xlab("The number of coefficients in the model")

```

# コードの参考
- https://yukiyanai.github.io/jp/classes/econometrics1/contents/R/multiple-comparison.html#%E5%A4%9A%E9%87%8D%E6%AF%94%E8%BC%83%E8%A3%9C%E6%AD%A3%E3%81%AE%E8%A8%88%E7%AE%97%E6%B3%95

# メモ
- chatGPTに教えてもらいました

切片の検定も多重比較なのか？
（1）一般的な多重比較とは異なる理由
多重比較の問題 とは、複数の仮説検定を行うことで偶然の有意性（Type I error）の増加を招くこと を指します。

しかし、回帰モデルの切片の検定は、他の係数の検定と独立した目的を持つため、通常の多重比較の問題とは扱いが異なります。

切片の検定: データの中心がゼロかどうかを調べる

A の検定: グループ間の差を検定する

そのため、多くの場合、切片の p 値は単なる情報提供として扱われ、通常の多重比較補正の対象にはなりません。

